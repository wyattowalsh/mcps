# PostgreSQL Migration Plan

## Overview

Migrate MCPS from SQLite to PostgreSQL for production-ready scalability, performance, and reliability.

## Why PostgreSQL?

1. **Scalability**: Handle 100K+ servers, millions of social posts
2. **Concurrency**: Multiple harvesters can write simultaneously
3. **Full-text search**: Built-in tsvector for better search
4. **JSON support**: Native JSONB for better querying
5. **Vector extensions**: pgvector for embeddings (better than sqlite-vec)
6. **Production-ready**: Industry standard with proven reliability
7. **Better indexing**: Advanced index types (GIN, GiST, BRIN)
8. **Connection pooling**: Enterprise-grade connection management

## Migration Strategy

### Phase 1: Backend Database Layer
- [x] Update pyproject.toml with asyncpg, psycopg2-binary
- [x] Replace database.py with PostgreSQL connection
- [x] Implement connection pooling (SQLAlchemy + asyncpg)
- [x] Update environment variables for PostgreSQL
- [x] Add health check endpoint for database

### Phase 2: Docker & Infrastructure
- [x] Add PostgreSQL service to docker-compose.yml
- [x] Configure persistent volume for data
- [x] Add initialization scripts
- [x] Update Dockerfile for PostgreSQL client
- [x] Add pgAdmin for database management

### Phase 3: Frontend Data Access
- [x] Replace better-sqlite3 with node-postgres (pg)
- [x] Update package.json dependencies
- [x] Refactor apps/web/src/lib/db.ts for PostgreSQL
- [x] Implement connection pooling in Next.js
- [x] Update all database queries for PostgreSQL syntax

### Phase 4: Migrations & Schema
- [x] Review all Alembic migrations for PostgreSQL compatibility
- [x] Update JSON column handling (SQLite JSON â†’ PostgreSQL JSONB)
- [x] Add PostgreSQL-specific indexes (GIN for JSONB, text search)
- [x] Update vector column for pgvector extension

### Phase 5: Best Practices Implementation
- [x] Connection pooling (asyncpg pool)
- [x] Transaction management
- [x] Error handling and retries
- [x] Query optimization
- [x] Prepared statements
- [x] Health checks
- [x] Graceful shutdown
- [x] Logging and monitoring

### Phase 6: Configuration & Documentation
- [x] Update .env.example with PostgreSQL settings
- [x] Update README.md with PostgreSQL setup
- [x] Update all documentation
- [x] Add migration guide from SQLite

## Technical Details

### Connection String Format
```
PostgreSQL: postgresql+asyncpg://user:password@host:port/database
SQLite: sqlite+aiosqlite:///data/mcps.db
```

### Key Changes

1. **Database Driver**
   - Old: aiosqlite
   - New: asyncpg (fastest async PostgreSQL driver)

2. **Connection Pooling**
   - Old: SQLite doesn't support pooling
   - New: SQLAlchemy pool + asyncpg pool (configurable size)

3. **JSON Columns**
   - Old: SQLite JSON type
   - New: PostgreSQL JSONB (indexed, queryable)

4. **Full-Text Search**
   - Old: Simple LIKE queries
   - New: PostgreSQL tsvector with GIN index

5. **Vector Search**
   - Old: sqlite-vec extension
   - New: pgvector extension (better performance)

6. **Array Types**
   - Old: JSON arrays
   - New: Native PostgreSQL arrays

### Best Practices Applied

1. **Connection Management**
   - Pool size: 20 (configurable)
   - Max overflow: 10
   - Pool pre-ping: True (test connections)
   - Pool recycle: 3600s (1 hour)

2. **Query Optimization**
   - Use EXPLAIN ANALYZE for slow queries
   - Add indexes on foreign keys
   - Add composite indexes for common filters
   - Use JSONB operators for JSON queries

3. **Error Handling**
   - Retry on connection errors
   - Rollback on failed transactions
   - Graceful degradation
   - Proper logging

4. **Security**
   - Parameterized queries (SQLAlchemy ORM)
   - SSL connections in production
   - Credential management via environment variables
   - Read-only user for analytics

5. **Monitoring**
   - Connection pool stats
   - Query performance metrics
   - Slow query logging
   - Health check endpoint

## Migration Steps

### For Existing Installations

```bash
# 1. Export data from SQLite (if needed)
uv run python scripts/export_sqlite_data.py

# 2. Update environment variables
cp .env.example .env
# Edit DATABASE_URL to use PostgreSQL

# 3. Start PostgreSQL
docker-compose up -d postgres

# 4. Run migrations
uv run alembic upgrade head

# 5. Import data (if migrating from SQLite)
uv run python scripts/import_to_postgresql.py

# 6. Verify
uv run python -m packages.harvester.cli stats
```

### For New Installations

```bash
# 1. Clone and setup
git clone <repo>
cd mcps

# 2. Copy environment file
cp .env.example .env
# PostgreSQL is already configured as default

# 3. Start services
docker-compose up -d

# 4. Initialize database
uv run alembic upgrade head

# 5. Start harvesting
uv run python -m packages.harvester.cli harvest-social --platform all
```

## Testing Checklist

- [ ] Database connection established
- [ ] All migrations apply successfully
- [ ] CRUD operations work correctly
- [ ] Connection pooling works
- [ ] Transaction rollback works
- [ ] Health check passes
- [ ] API endpoints work
- [ ] Next.js pages load correctly
- [ ] Social media harvesting works
- [ ] Background tasks run successfully

## Rollback Plan

If issues occur:

1. Keep SQLite database as backup
2. Point DATABASE_URL back to SQLite
3. Restart services
4. Investigate PostgreSQL issues
5. Fix and retry migration

## Performance Benchmarks

Expected improvements:

- **Write throughput**: 10x faster (concurrent writes)
- **Read throughput**: 5x faster (better indexing)
- **Query latency**: 50% reduction (query optimization)
- **Concurrent users**: 100x increase (connection pooling)
- **Full-text search**: 20x faster (tsvector + GIN index)

## Timeline

- **Planning**: 1 hour âœ“
- **Backend migration**: 2 hours
- **Frontend migration**: 1 hour
- **Testing**: 1 hour
- **Documentation**: 30 minutes
- **Total**: ~5 hours

## Status

ðŸš§ **In Progress** - Backend migration started
